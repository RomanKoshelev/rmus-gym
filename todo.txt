TODO:

- store learning episodes number in th.var and restore it
- save/load the reply buffer
- save reward to tensorboard
- Test the Algorithm on current Mujoco-based OpenAI-Gym environment (should learn in appropriate time)


DONE:
+ fix qqpg
+ use clipping for actions (tanh ativation)
+ use action_size for for noise (see loanaCheu/AgentDDPG line 17)
+ Make test file
+ draw vector to goal
+ use scope for saving: http://stackoverflow.com/questions/36533723/tensorflow-get-all-variables-in-scope
+ train and save with tf.scopes
+ implement wrapper agent
+ refactor: use agent class
  + create
  + train
  + load
  + save
  + run
+ add train scripts
+ test ioanachelu's agent (https://github.com/ioanachelu/turi_ddpg)
+ test stevenpjg's agent
+ limit GPU memory usage for the TF session to run several tests simultatiously
+ add big reward for the touching
+ implement Brick env
  + https://gym.openai.com/evaluations/eval_xjuUFdvrQR68YWvUqsjKPQ#writeup
  + https://gym.openai.com/evaluations/eval_i3lTYeUTW27Q0bBAjAvSw
  + https://gym.openai.com/evaluations/eval_RE77KlTNTvCGzTgtm0Qqg
  + https://github.com/SimonRamstedt/ddpg
  + https://github.com/songrotek/DDPG
+ implement alg:
  + find and read TNPG
  + find and read TRPO
+ select alg
+ read
 + http://arxiv.org/pdf/1509.02971v5.pdf
 + actor-critic approach


LATER:
- add h1 h2 to DDGP args
- add nn arch to scope name (_12x100x400x45_)
- in superagent: use whole agent's observations
- add cost for sharp junction angles
- add more segments
- create spiders
- add colloborative environment
- read:
 - https://www.dropbox.com/s/rqtpp1jv2jtzxeg/ICML2016_benchmarking_slides.pdf?dl=0
 - http://arxiv.org/pdf/1511.06581v3.pdf
- use batch normalization

view:
- http://videolectures.net/rldm2015_silver_reinforcement_learning/
- selected alg

- experiments:
  + wrapper
  - two hands
  - see diagramm
- improve env model
  - add cost for sharp junction angles
  - add to obs: explicit target and head coords

- learning performance
  - decade LR
  - batch normalization
  - multy env learning
  - dual network
