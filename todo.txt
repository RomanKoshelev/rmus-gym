TODO:

- use [0..1] for all inputs and outputs
  - rewrite env
  - rewrite alg
  - add clips
  - add asserts
- use whole agent's observations
  - add h1 h2 to DQNN args
  - add nn arch to scope name (_12x100x400x45_)
- experiments:
  + wrapper
  - two hands
  - see diagramm
- improve env model
  - add cost for sharp junction angles
  - add to obs: explicit target and head coords

- learning performance
  - decade LR
  - store learning episodes number in th.var and restore it
  - save/load the reply buffer
  - batch normalization
  - multy env learning
  - dual network


DESCRIPTION:
- IDEA:
  - reuse pretrained skills,
  - compose superagent from ready-to-use units
  - learn superagent's driver to manipulate with units
  - method: warp unit's observations,
  - problem: detect and isolate external and internal observations
- PROFIT:
  - rapid learning
  - reusable components
  - introducing compensation to achive goals by new ways open by driver
- CHALENGE:
  - utilize the human-mentors intuition to improve the RL preformance
  - separate ext/eig observations to generate explicit tasks between units
  - learn by samples -- looking for profy and try to repeat the solution by self (mirror neurons)
- QUESTIONS:
  - which alg use for the superagent's driver
  - how to speed up the unit learning
  - how to obtain the feedback from unit
  - how to train unit simultaniously with its driver
  - how to avoid local minimum (bad habits) e.g. far goals instead of close ones when the real goal is near a floor
  - add motivation to driver -- avoid goals' trimmer
  - other motivations?
  - problem -- need for quick training to make more experiments. How to solve?
  - must driver have more large hidden layers then the unit's net driven by it?
  - transfer the skills as weight from one agent to other (from arm to leg) and tune via driver
  - how much should be the noise range relatevely to the action signal
  - how to normalize the actions and obs to make them laying in the [0..1] interval
  - how to hint the agent what to do (m/b via motivation?)
  - how to tune the loss function automatically

DONE:
+ draw vector to goal
+ use scope for saving: http://stackoverflow.com/questions/36533723/tensorflow-get-all-variables-in-scope
+ train and save with tf.scopes
+ implement wrapper agent
+ refactor: use agent class
  + create
  + train
  + load
  + save
  + run
+ add train scripts
+ test ioanachelu's agent (https://github.com/ioanachelu/turi_ddpg)
+ test stevenpjg's agent
+ limit GPU memory usage for the TF session to run several tests simultatiously
+ add big reward for the touching
+ implement Brick env
  + https://gym.openai.com/evaluations/eval_xjuUFdvrQR68YWvUqsjKPQ#writeup
  + https://gym.openai.com/evaluations/eval_i3lTYeUTW27Q0bBAjAvSw
  + https://gym.openai.com/evaluations/eval_RE77KlTNTvCGzTgtm0Qqg
  + https://github.com/SimonRamstedt/ddpg
  + https://github.com/songrotek/DDPG
+ implement alg:
  + find and read TNPG
  + find and read TRPO
+ select alg
+ read
 + http://arxiv.org/pdf/1509.02971v5.pdf
 + actor-critic approach


LATER:
- add cost for sharp junction angles
- add more segments
- create spiders
- add colloborative environment
- read:
 - https://www.dropbox.com/s/rqtpp1jv2jtzxeg/ICML2016_benchmarking_slides.pdf?dl=0
 - http://arxiv.org/pdf/1511.06581v3.pdf
- use batch normalization

view:
- http://videolectures.net/rldm2015_silver_reinforcement_learning/
- selected alg

