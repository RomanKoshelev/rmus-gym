TODO:

> use whole agent's observations
  - add dims to scope name (_12x45_)
- draw vector to goal
- experiments:
  - wrapper
  - two hands
  - see diagramm

DONE:
+ use scope for saving: http://stackoverflow.com/questions/36533723/tensorflow-get-all-variables-in-scope
+ train and save with tf.scopes
+ implement wrapper agent
+ refactor: use agent class
  + create
  + train
  + load
  + save
  + run
+ add train scripts
+ test ioanachelu's agent (https://github.com/ioanachelu/turi_ddpg)
+ test stevenpjg's agent
+ limit GPU memory usage for the TF session to run several tests simultatiously
+ add big reward for the touching
+ implement Brick env
  + https://gym.openai.com/evaluations/eval_xjuUFdvrQR68YWvUqsjKPQ#writeup
  + https://gym.openai.com/evaluations/eval_i3lTYeUTW27Q0bBAjAvSw
  + https://gym.openai.com/evaluations/eval_RE77KlTNTvCGzTgtm0Qqg
  + https://github.com/SimonRamstedt/ddpg
  + https://github.com/songrotek/DDPG
+ implement alg:
  + find and read TNPG
  + find and read TRPO
+ select alg
+ read
 + http://arxiv.org/pdf/1509.02971v5.pdf
 + actor-critic approach


LATER:
- add cost for sharp junction angles
- add more segments
- create spiders
- add colloborative environment
- read:
 - https://www.dropbox.com/s/rqtpp1jv2jtzxeg/ICML2016_benchmarking_slides.pdf?dl=0
 - http://arxiv.org/pdf/1511.06581v3.pdf
- use batch normalization

view:
- http://videolectures.net/rldm2015_silver_reinforcement_learning/
- selected alg

